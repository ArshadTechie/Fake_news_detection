{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f827529",
   "metadata": {},
   "source": [
    "## Importing libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7ca197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Obtaining dependency information for wordcloud from https://files.pythonhosted.org/packages/f5/b0/247159f61c5d5d6647171bef84430b7efad4db504f0229674024f3a4f7f2/wordcloud-1.9.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading wordcloud-1.9.3-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arsha\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Downloading wordcloud-1.9.3-cp311-cp311-win_amd64.whl (300 kB)\n",
      "   ---------------------------------------- 0.0/300.2 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/300.2 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/300.2 kB 495.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 174.1/300.2 kB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 204.8/300.2 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 300.2/300.2 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f16b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer\n",
    "from wordcloud import WordCloud\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26be1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\arsha\\Downloads\\archive (1)\\fakenews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aadbfca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get the latest from TODAY Sign up for our news...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2d  Conan On The Funeral Trump Will Be Invited...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s safe to say that Instagram Stories has fa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Much like a certain Amazon goddess with a lass...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At a time when the perfect outfit is just one ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Get the latest from TODAY Sign up for our news...      1\n",
       "1  2d  Conan On The Funeral Trump Will Be Invited...      1\n",
       "2  It’s safe to say that Instagram Stories has fa...      0\n",
       "3  Much like a certain Amazon goddess with a lass...      0\n",
       "4  At a time when the perfect outfit is just one ...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0b4277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your data contains lower and upper case\n",
      "Your data contains HTML tags\n",
      "Your data contains URLs\n",
      "Your data contains hashtags\n",
      "Your data contains mentions\n",
      "Your data contains unwanted characters\n",
      "Your data contains emojis\n"
     ]
    }
   ],
   "source": [
    "# EDA\n",
    "def eda(data, column):\n",
    "    lower = ' '.join(data[column]).islower()\n",
    "    html = data[column].apply(lambda x: True if re.search('<.*?>',x) else False).sum()\n",
    "    urls = data[column].apply(lambda x: True if re.search('http[s]?://.+?\\S+',x) else False).sum()\n",
    "    hasht = data[column].apply(lambda x: True if re.search('#\\S+',x) else False).sum()\n",
    "    mentions = data[column].apply(lambda x: True if re.search('@\\S+',x) else False).sum()\n",
    "    un_c = data[column].apply(lambda x: True if re.search(\"[]\\.\\*'\\-#@$%^?~`!&,(0-9)]\",x) else False).sum()\n",
    "    emojiss = data[column].apply(lambda x: True if emoji.emoji_count(x) else False).sum()\n",
    "    if not lower:\n",
    "        print('Your data contains lower and upper case')\n",
    "    if html > 0:\n",
    "        print(\"Your data contains HTML tags\")\n",
    "    if urls > 0:\n",
    "        print(\"Your data contains URLs\")\n",
    "    if hasht > 0:\n",
    "        print(\"Your data contains hashtags\")\n",
    "    if mentions > 0:\n",
    "        print(\"Your data contains mentions\")\n",
    "    if un_c:\n",
    "        print(\"Your data contains unwanted characters\")\n",
    "    if emojiss:\n",
    "        print(\"Your data contains emojis\")\n",
    "\n",
    "eda(df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636778a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Splitting the dataset\n",
    "fv = df[\"text\"]\n",
    "cv = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acaece1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(fv, cv, test_size=0.2, random_state=1, stratify=cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f1971b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_pp(x, emoj=\"F\"):\n",
    "    if emoj == \"T\":\n",
    "        x = emoji.demojize(x)\n",
    "    x = x.lower()\n",
    "    x = re.sub('<.*?>', ' ', x)\n",
    "    x = re.sub('http[s]?://.+?\\S+', ' ', x)\n",
    "    x = re.sub('#\\S+', ' ', x)\n",
    "    x = re.sub('@\\S+', ' ', x)\n",
    "    x = re.sub(\"[]\\.\\*'’‘_—,:{}\\-#@$%^?~`!&(0-9)]\", ' ', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab95ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmat(x):\n",
    "    sent = []\n",
    "    ls = LancasterStemmer()\n",
    "    for word in word_tokenize(x):\n",
    "        sent.append(ls.stem(word))\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb7b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_p = x_train.apply(basic_pp, args=(\"T\",)).apply(lemmat)\n",
    "x_test_p = x_test.apply(basic_pp, args=(\"T\",)).apply(lemmat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3ee35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbow = CountVectorizer(binary=True)\n",
    "x_train_pf = bbow.fit_transform(x_train_p)\n",
    "x_test_pf = bbow.transform(x_test_p)\n",
    "# saving okl\n",
    "with open(r'C:\\Users\\arsha\\OneDrive\\Desktop\\pickle\\countvectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(bbow,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0299c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bernoulli naive bayes\n",
    "bnb = BernoulliNB(alpha=1)\n",
    "pkl= bnb.fit(x_train_pf, y_train)\n",
    "with open(r'C:\\Users\\arsha\\OneDrive\\Desktop\\pickle\\bernoulli.pkl', 'wb') as f:\n",
    "    pickle.dump(pkl,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d23df698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial NB\n",
    "B_O_W =  CountVectorizer()\n",
    "x_train_pf = B_O_W .fit_transform(x_train_p)\n",
    "x_test_pf = B_O_W .transform(x_test_p)\n",
    "\n",
    "# Save Multinomial NB model\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "pkl = mnb.fit(x_train_pf, y_train)\n",
    "with open(r'C:\\Users\\arsha\\OneDrive\\Desktop\\pickle\\multinomial.pkl', 'wb') as f:\n",
    "    pickle.dump(pkl, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b5aa50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "x_train_pf = tfidf.fit_transform(x_train_p)\n",
    "x_test_pf = tfidf.transform(x_test_p)\n",
    "# Save TF-IDF model\n",
    "with open(r'C:\\Users\\arsha\\OneDrive\\Desktop\\pickle\\TFIDF.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81b88f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Multinomial NB model using TF-IDF\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "model = mnb.fit(x_train_pf, y_train)\n",
    "with open(r'C:\\Users\\arsha\\OneDrive\\Desktop\\pickle\\multinomialtf.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "403a79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN using BOW\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "model = knn.fit(x_train_pf, y_train)\n",
    "# Save KNN model using Bag of Words\n",
    "with open(r'C:\\Users\\arsha\\OneDrive\\Desktop\\pickle\\knnBOW.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99b35a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn with TF-IDF\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "model = knn.fit(x_train_pf, y_train)\n",
    "# Save KNN model\n",
    "with open(r'C:\\Users\\arsha\\OneDrive\\Desktop\\pickle\\knnTFIDF.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd1085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
